{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import fitz\n",
    "import random\n",
    "import re\n",
    "import psutil\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "from spacy.lang.en import English\n",
    "from time import perf_counter as timer \n",
    "import textwrap\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from transformers import BitsAndBytesConfig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File human-nutrition-text.pdf exists.\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"human-nutrition-text.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "  print(\"File doesn't exist, downloading...\")\n",
    "\n",
    "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
    "\n",
    "  filename = pdf_path\n",
    "\n",
    "  response = requests.get(url)\n",
    "\n",
    "  if response.status_code == 200:\n",
    "      with open(filename, \"wb\") as file:\n",
    "          file.write(response.content)\n",
    "      print(f\"The file has been downloaded and saved as {filename}\")\n",
    "  else:\n",
    "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "else:\n",
    "  print(f\"File {pdf_path} exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd60fd53b57b4f3682a8a39bc867bf56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': -41,\n",
       "  'page_char_count': 29,\n",
       "  'page_word_count': 4,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 7.25,\n",
       "  'text': 'Human Nutrition: 2020 Edition'},\n",
       " {'page_number': -40,\n",
       "  'page_char_count': 0,\n",
       "  'page_word_count': 1,\n",
       "  'page_sentence_count_raw': 1,\n",
       "  'page_token_count': 0.0,\n",
       "  'text': ''}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def text_formatter(text: str) -> str:\n",
    "    \"\"\"Performs minor formatting on text.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"\n",
    "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
    "\n",
    "    Parameters:\n",
    "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
    "\n",
    "    Returns:\n",
    "        list[dict]: A list of dictionaries, each containing the page number\n",
    "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
    "        for each page.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)  \n",
    "    pages_and_texts = []\n",
    "    for page_number, page in tqdm(enumerate(doc)): \n",
    "        text = page.get_text() \n",
    "        text = text_formatter(text)\n",
    "        pages_and_texts.append({\"page_number\": page_number - 41,  \n",
    "                                \"page_char_count\": len(text),\n",
    "                                \"page_word_count\": len(text.split(\" \")),\n",
    "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
    "                                \"page_token_count\": len(text) / 4,  # https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
    "                                \"text\": text})\n",
    "    return pages_and_texts\n",
    "\n",
    "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
    "pages_and_texts[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "1          -40                0                1                        1   \n",
       "2          -39              320               54                        1   \n",
       "3          -38              212               32                        1   \n",
       "4          -37              797              147                        3   \n",
       "\n",
       "   page_token_count                                               text  \n",
       "0              7.25                      Human Nutrition: 2020 Edition  \n",
       "1              0.00                                                     \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...  \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...  \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34b0bf81e2d47d1994ae9e0bbbedf01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = English()\n",
    "nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
    "    \n",
    "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
    "    \n",
    "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "      <td>[Human Nutrition: 2020 Edition]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "      <td>[Human Nutrition: 2020  Edition  UNIVERSITY OF...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>[Human Nutrition: 2020 Edition by University o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "      <td>[Contents  Preface  University of Hawai‘i at M...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "1          -40                0                1                        1   \n",
       "2          -39              320               54                        1   \n",
       "3          -38              212               32                        1   \n",
       "4          -37              797              147                        3   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0              7.25                      Human Nutrition: 2020 Edition   \n",
       "1              0.00                                                      \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...   \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...   \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0                    [Human Nutrition: 2020 Edition]   \n",
       "1                                                 []   \n",
       "2  [Human Nutrition: 2020  Edition  UNIVERSITY OF...   \n",
       "3  [Human Nutrition: 2020 Edition by University o...   \n",
       "4  [Contents  Preface  University of Hawai‘i at M...   \n",
       "\n",
       "   page_sentence_count_spacy  \n",
       "0                          1  \n",
       "1                          0  \n",
       "2                          1  \n",
       "3                          1  \n",
       "4                          2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sentence_chunk_size = 10\n",
    "\n",
    "def split_list(input_list: list,\n",
    "               slice_size: int = num_sentence_chunk_size) -> list[list[str]]:\n",
    "    \n",
    "    return [input_list[i:i+slice_size] for i in range(0, len(input_list), slice_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "148b13308f584aaea48b37458de593f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for item in tqdm(pages_and_texts):\n",
    "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
    "                                         slice_size=num_sentence_chunk_size)\n",
    "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>page_char_count</th>\n",
       "      <th>page_word_count</th>\n",
       "      <th>page_sentence_count_raw</th>\n",
       "      <th>page_token_count</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>page_sentence_count_spacy</th>\n",
       "      <th>sentence_chunks</th>\n",
       "      <th>num_chunks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7.25</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "      <td>[Human Nutrition: 2020 Edition]</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Human Nutrition: 2020 Edition]]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-40</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-39</td>\n",
       "      <td>320</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>80.00</td>\n",
       "      <td>Human Nutrition: 2020  Edition  UNIVERSITY OF ...</td>\n",
       "      <td>[Human Nutrition: 2020  Edition  UNIVERSITY OF...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Human Nutrition: 2020  Edition  UNIVERSITY O...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-38</td>\n",
       "      <td>212</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>53.00</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>[Human Nutrition: 2020 Edition by University o...</td>\n",
       "      <td>1</td>\n",
       "      <td>[[Human Nutrition: 2020 Edition by University ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-37</td>\n",
       "      <td>797</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "      <td>199.25</td>\n",
       "      <td>Contents  Preface  University of Hawai‘i at Mā...</td>\n",
       "      <td>[Contents  Preface  University of Hawai‘i at M...</td>\n",
       "      <td>2</td>\n",
       "      <td>[[Contents  Preface  University of Hawai‘i at ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number  page_char_count  page_word_count  page_sentence_count_raw  \\\n",
       "0          -41               29                4                        1   \n",
       "1          -40                0                1                        1   \n",
       "2          -39              320               54                        1   \n",
       "3          -38              212               32                        1   \n",
       "4          -37              797              147                        3   \n",
       "\n",
       "   page_token_count                                               text  \\\n",
       "0              7.25                      Human Nutrition: 2020 Edition   \n",
       "1              0.00                                                      \n",
       "2             80.00  Human Nutrition: 2020  Edition  UNIVERSITY OF ...   \n",
       "3             53.00  Human Nutrition: 2020 Edition by University of...   \n",
       "4            199.25  Contents  Preface  University of Hawai‘i at Mā...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0                    [Human Nutrition: 2020 Edition]   \n",
       "1                                                 []   \n",
       "2  [Human Nutrition: 2020  Edition  UNIVERSITY OF...   \n",
       "3  [Human Nutrition: 2020 Edition by University o...   \n",
       "4  [Contents  Preface  University of Hawai‘i at M...   \n",
       "\n",
       "   page_sentence_count_spacy  \\\n",
       "0                          1   \n",
       "1                          0   \n",
       "2                          1   \n",
       "3                          1   \n",
       "4                          2   \n",
       "\n",
       "                                     sentence_chunks  num_chunks  \n",
       "0                  [[Human Nutrition: 2020 Edition]]           1  \n",
       "1                                                 []           0  \n",
       "2  [[Human Nutrition: 2020  Edition  UNIVERSITY O...           1  \n",
       "3  [[Human Nutrition: 2020 Edition by University ...           1  \n",
       "4  [[Contents  Preface  University of Hawai‘i at ...           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_texts)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a610054932f14fc89ac80fb619097a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1208 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1843"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_and_chunks = []\n",
    "\n",
    "for item in tqdm(pages_and_texts):\n",
    "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
    "        chunk_dict = {}\n",
    "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
    "        \n",
    "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
    "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) \n",
    "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
    "        \n",
    "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
    "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
    "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
    "        \n",
    "        pages_and_chunks.append(chunk_dict)\n",
    "\n",
    "len(pages_and_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-41</td>\n",
       "      <td>Human Nutrition: 2020 Edition</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>7.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>116</td>\n",
       "      <td>191.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>144</td>\n",
       "      <td>235.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -41                      Human Nutrition: 2020 Edition   \n",
       "1          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "2          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "3          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "4          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \n",
       "0                29                 4               7.25  \n",
       "1               308                42              77.00  \n",
       "2               210                30              52.50  \n",
       "3               766               116             191.50  \n",
       "4               941               144             235.25  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(pages_and_chunks)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk token count: 16.0 | Text: Accessed January 20, 2018. 1032 | The Effect of New Technologies\n",
      "chunk token count: 17.0 | Text: Figure 2.15 Major Respiratory Structures The Respiratory System | 99\n",
      "chunk token count: 9.5 | Text: 742 | Building Healthy Eating Patterns\n",
      "chunk token count: 10.75 | Text: Accessed December 10, 2017. 880 | Childhood\n",
      "chunk token count: 28.75 | Text: Bouayed, J. and T. Bohn. (2010). Exogenous Antioxidants—Double-Edged Swords in Cellular Redox MyPlate Planner | 753\n"
     ]
    }
   ],
   "source": [
    "min_token_length = 30\n",
    "\n",
    "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
    "    print(f\"chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_and_chunks_over_min_token_length = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'page_number': 107,\n",
       "  'sentence_chunk': 'in the endocrine system are the pituitary, thyroid, parathyroid, adrenals, thymus, pineal, pancreas, ovaries, and testes. The glands secrete hormones, which are biological molecules that regulate cellular processes in other target tissues, so they require transportation by the circulatory system. Adequate nutrition is critical for the functioning of all the glands in the endocrine system. A protein deficiency impairs gonadal-hormone release, preventing reproduction. Athletic teenage girls with very little body fat often do not menstruate. Children who are malnourished usually do not produce enough growth hormone and fail to reach normal height for their age group. Probably the most popularized connection between nutrition and the functions of the endocrine system is that unhealthy dietary patterns are linked to obesity and the development of Type 2 diabetes. The Centers for Disease Control and Prevention (CDC) estimates that twenty-six million Americans have Type 2 diabetes as of 2011. This is 8.3 percent of the US population. Counties with the highest incidence of obesity also have the highest incidence of Type 2 diabetes.',\n",
       "  'chunk_char_count': 1141,\n",
       "  'chunk_word_count': 170,\n",
       "  'chunk_token_count': 285.25}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(pages_and_chunks_over_min_token_length, k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDING THE TEXT CHUNCKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berkan_oztas/Bitcoin/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/Users/berkan_oztas/Bitcoin/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf779d4faf014b52a5729c8a633f5252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.52156121e-02  5.92140220e-02 -1.66167002e-02 -2.04602312e-02\n",
      "  6.92423135e-02  3.51346135e-02 -1.87619645e-02  3.21568623e-02\n",
      "  7.78691024e-02 -8.06521624e-03  2.60772090e-02  1.17807147e-04\n",
      "  2.36337334e-02  6.99440809e-03  1.76008643e-06 -3.82591551e-03\n",
      "  3.45729385e-03  1.16404612e-02  1.01687415e-02  4.95471694e-02\n",
      " -5.18356450e-02  1.88298319e-02  4.51909713e-02  4.23135012e-02\n",
      " -4.12121825e-02  4.93987091e-03  3.25199589e-02 -1.81734581e-02\n",
      "  8.84532649e-03 -6.44744113e-02 -5.04505588e-03  1.74673516e-02\n",
      " -1.65685068e-03 -8.50824863e-02  2.46762693e-06 -1.69053245e-02\n",
      "  1.09408684e-02  3.01258136e-02 -6.66744560e-02  6.21617064e-02\n",
      "  3.50563154e-02 -2.47929841e-02 -1.59021597e-02  2.37372015e-02\n",
      "  3.93133499e-02  4.06050757e-02  4.51445505e-02 -5.83526306e-03\n",
      " -1.52490400e-02  8.62988178e-03 -1.96103961e-03 -3.10199130e-02\n",
      " -3.25587057e-02  2.62657250e-03  3.76190506e-02  3.28164026e-02\n",
      " -1.42093375e-02  1.82541087e-02  4.01719799e-03 -4.65871580e-02\n",
      "  4.56143077e-03  5.84205762e-02  2.41824146e-02  4.43892889e-02\n",
      " -3.36209871e-02  4.68510054e-02  1.64948534e-02 -8.93779173e-02\n",
      "  5.91009296e-02  3.92325409e-02  5.59264682e-02  3.62490676e-02\n",
      " -1.26896212e-02  2.66754776e-02 -2.30123661e-02 -2.57726237e-02\n",
      "  3.15983109e-02 -4.82672080e-02 -1.00867525e-02 -2.26086937e-02\n",
      " -1.84208788e-02  1.81875359e-02  1.87498946e-02 -4.41873819e-03\n",
      " -9.78095829e-03  4.54946384e-02 -2.52456008e-03  2.63714194e-02\n",
      " -1.59761086e-02 -4.73841317e-02 -2.53286976e-02  4.54883091e-02\n",
      "  1.16945468e-02  1.95936523e-02  1.59996059e-02 -3.19498777e-02\n",
      "  7.67783821e-02 -5.63971736e-02  7.72040486e-02 -1.18497480e-02\n",
      "  2.81400625e-02  5.30569106e-02 -7.89483562e-02 -1.37166120e-02\n",
      "  9.39890090e-03 -3.42182582e-03  2.94115935e-02 -2.16187779e-02\n",
      " -3.75934169e-02  5.42530045e-02 -9.44305956e-03  2.64974864e-04\n",
      "  1.69651199e-03 -2.23166030e-02  2.80579622e-03  1.63756665e-02\n",
      " -8.04615673e-03 -3.24969031e-02 -2.55579688e-02 -1.00079753e-01\n",
      "  6.08365517e-03  3.77003700e-02 -1.80503633e-02  5.35653196e-02\n",
      " -5.03368825e-02  8.70372429e-02 -1.12762413e-04  1.43129833e-03\n",
      "  6.47684792e-03 -6.05411902e-02  5.72265126e-03 -2.83855032e-02\n",
      " -4.12982628e-02  6.93741627e-03  3.78072145e-03 -4.00167424e-03\n",
      " -4.74617031e-04  1.43406140e-02  3.97382677e-03 -5.98132387e-02\n",
      " -5.67999706e-02 -1.20047526e-02 -6.50709122e-02 -4.73855669e-03\n",
      "  9.38248187e-02  3.24579817e-03  7.57833347e-02 -1.54256402e-02\n",
      "  2.10188963e-02  7.15474784e-02 -8.87704268e-03 -4.60221954e-02\n",
      "  4.34302539e-02  7.16091394e-02  9.67703015e-02  1.31542003e-03\n",
      "  2.28222329e-02 -3.94847244e-02  5.43755330e-02  5.94021603e-02\n",
      " -1.26992613e-02 -2.00304687e-02 -8.27437490e-02 -1.54699767e-02\n",
      " -1.11956252e-02  4.19065952e-02  1.60137825e-02 -6.09247107e-03\n",
      "  1.22761400e-02  1.87997706e-02 -5.44637702e-02 -1.92611106e-02\n",
      " -2.35970505e-02  1.29548721e-02  5.29097877e-02  2.03884263e-02\n",
      " -3.41150388e-02  9.29078739e-03  5.39537892e-02 -1.93090271e-02\n",
      "  4.11308296e-02 -5.41311353e-02  1.22171231e-02 -2.41647518e-04\n",
      " -5.21566086e-02 -1.08857360e-02  1.48703875e-02  8.66235513e-03\n",
      "  5.04872352e-02 -2.80269273e-02  3.96374539e-02  1.58218406e-02\n",
      " -1.02065811e-02 -9.01903659e-02 -4.21214942e-03  2.82837427e-03\n",
      "  1.73775684e-02 -3.63479229e-03  3.65830846e-02 -7.47961625e-02\n",
      "  5.40357865e-02  3.33905481e-02  4.70152870e-02 -1.08505236e-02\n",
      " -1.36760967e-02  2.36349050e-02  4.02396657e-02 -3.98386903e-02\n",
      "  1.76650789e-02 -3.80105600e-02 -7.32314959e-02 -1.66221485e-02\n",
      "  5.59994876e-02  5.82900643e-03 -2.08013207e-02  5.11073833e-03\n",
      " -2.00578831e-02  2.35285517e-02  1.44078359e-02  9.52305868e-02\n",
      "  4.85367253e-02  3.70356515e-02 -4.86390181e-02 -1.07203675e-02\n",
      " -4.37168032e-02 -8.05551279e-03  4.48279642e-02 -2.01690923e-02\n",
      "  7.69364135e-03  3.78706008e-02  3.88277322e-02  6.18671440e-02\n",
      " -3.09114791e-02  3.42814089e-03 -1.82494253e-03  3.64127643e-02\n",
      "  5.56131788e-02  7.10415766e-02 -2.92531867e-02  3.93743515e-02\n",
      " -6.76081032e-02 -1.17224436e-02 -3.47777605e-02 -1.52253488e-03\n",
      " -6.79490939e-02  4.54261992e-03 -2.63378359e-02 -1.24772284e-02\n",
      " -6.77631199e-02  7.32906684e-02  3.29785161e-02 -9.23928153e-03\n",
      " -1.67754684e-02  2.03387272e-02 -6.94909319e-02 -1.31008625e-02\n",
      " -4.70621586e-02 -4.09856550e-02 -2.89755277e-02  6.44979551e-02\n",
      " -4.65418287e-02  3.75503525e-02  2.26722788e-02  2.10691220e-03\n",
      "  2.75564156e-02 -8.29299837e-02  4.90500554e-02  2.09571375e-03\n",
      "  9.57552250e-03 -2.62901746e-02  2.66504334e-03 -3.78408134e-02\n",
      " -2.27989275e-02  2.15301495e-02 -7.05231074e-03  3.97109091e-02\n",
      "  2.32817624e-02 -5.92527352e-02  3.83276418e-02 -4.05594744e-02\n",
      " -1.65522117e-02  7.34763145e-02 -7.58592598e-03  2.80700494e-02\n",
      "  3.42205018e-02 -1.50588807e-02  5.29853022e-03 -1.37232272e-02\n",
      "  8.01459476e-02 -4.44525853e-02  5.32305241e-02 -5.11693545e-02\n",
      "  1.96508989e-02 -6.76708156e-03 -2.88854912e-02  4.36495394e-02\n",
      " -6.05717627e-03  6.81037200e-04  3.03577874e-02 -4.02853563e-02\n",
      "  3.39221791e-03  2.36008391e-02 -4.18885751e-03  4.29780930e-02\n",
      " -1.80875827e-02  4.36334237e-02 -2.68290322e-02 -5.30372746e-02\n",
      "  9.17065982e-03 -2.36257501e-02 -4.11054119e-03  6.75034076e-02\n",
      "  1.18485214e-02  2.28240080e-02 -8.46262462e-03 -2.78143436e-02\n",
      " -4.36786674e-02  3.44559178e-02  2.24191844e-02  2.43157521e-03\n",
      "  1.54484073e-02 -5.91077702e-03 -5.17940149e-03  1.35786803e-02\n",
      " -5.98328374e-03  1.62526628e-03  3.46854851e-02  3.36104706e-02\n",
      " -9.90929306e-02 -5.21445908e-02 -1.32977744e-04 -6.12494443e-03\n",
      "  5.47548383e-02 -1.35317538e-02  8.29972140e-03 -3.92724238e-02\n",
      " -3.32509689e-02  4.86705638e-03 -5.18718250e-02 -6.31548762e-02\n",
      " -4.25764509e-02 -3.00455955e-03  1.02959666e-02  5.16110733e-02\n",
      " -1.56971402e-02 -3.14593203e-02  2.83667408e-02  2.71748155e-02\n",
      " -1.34294499e-02  1.58371627e-02 -3.64916511e-02 -6.92466125e-02\n",
      "  3.42160016e-02 -8.34293291e-03  1.53991040e-02 -2.04658713e-02\n",
      "  2.99733914e-02 -6.05815032e-04 -6.47166185e-03  7.09841214e-03\n",
      "  2.15781759e-02  7.40466043e-02 -1.42913833e-02 -7.03953058e-02\n",
      "  6.89639524e-02  3.14483903e-02 -2.47505624e-02  4.25217748e-02\n",
      "  3.46465260e-02 -3.51549648e-02  8.48862343e-03  6.49516433e-02\n",
      " -1.51940137e-02 -3.85348988e-03  1.56368129e-02 -2.81493161e-02\n",
      "  3.46453115e-02  4.89952601e-02 -5.80370687e-02 -2.93343738e-02\n",
      "  4.66520227e-02  3.00583895e-03  3.94278951e-02  9.65769496e-03\n",
      " -1.66797228e-02 -3.21641117e-02 -4.13096473e-02 -3.26444730e-02\n",
      "  2.50301193e-02 -4.35861796e-02  2.86592152e-02 -1.13898423e-02\n",
      " -1.69067010e-02 -8.92129354e-03  1.97629575e-02 -1.22486369e-03\n",
      "  3.43902297e-02 -2.82936916e-02 -1.24840178e-02  3.09178680e-02\n",
      " -1.30081270e-02 -3.18479761e-02  4.83042710e-02  3.37061398e-02\n",
      " -6.52791420e-03  5.25808614e-03  2.03737356e-02  2.65504476e-02\n",
      "  3.33993696e-02  1.76927485e-02  2.28342190e-02 -3.16658020e-02\n",
      "  1.22985151e-02 -5.65573536e-02  1.93445589e-02  2.14187354e-02\n",
      "  2.22393847e-03 -3.16535346e-02  3.38878296e-02 -4.15002108e-02\n",
      " -5.29043600e-02  4.35204618e-02 -1.00048389e-02  4.57410477e-02\n",
      " -3.97950076e-02 -1.75691713e-02 -2.12071755e-04 -3.50252981e-03\n",
      " -5.99535517e-02  3.15569378e-02  3.71586792e-02  1.53412642e-02\n",
      " -3.25054047e-03 -3.03750653e-02 -1.33599108e-02  1.15642007e-02\n",
      "  7.02178404e-02 -5.78267239e-02 -4.57702130e-02 -2.26734597e-02\n",
      "  3.45159322e-04  8.17545597e-03  8.74054246e-03 -1.65661387e-02\n",
      "  1.60023384e-02  1.17951212e-02 -1.08058555e-02  6.37831017e-02\n",
      "  9.50481296e-02  4.32757288e-02  9.10396781e-03  3.41964550e-02\n",
      " -1.44445514e-02  6.20703548e-02  1.93755310e-02  7.05717946e-04\n",
      "  2.35212203e-02 -4.79264893e-02  1.96139961e-02 -7.29679083e-03\n",
      "  5.35638034e-02 -1.08612888e-02 -1.61705539e-04  1.89436842e-02\n",
      " -2.73146257e-02 -1.22281909e-02 -5.22387726e-03  2.17847787e-02\n",
      " -4.84843329e-02 -3.02606672e-02 -3.07689961e-02  4.98095527e-02\n",
      "  5.22567006e-03  1.09676132e-03  1.01793138e-02  2.78311991e-03\n",
      " -6.36349097e-02 -1.92798246e-02 -1.46244606e-02  3.26665528e-02\n",
      " -2.56526042e-02  3.08137969e-03 -1.35248536e-02 -1.25948284e-02\n",
      "  1.18030433e-03  3.14961234e-03 -7.06644403e-03 -3.59070376e-02\n",
      " -8.75007303e-04 -5.86845428e-02  2.76067536e-02  3.43779922e-02\n",
      " -1.17734475e-02 -3.74605618e-02 -2.59200335e-02  3.02399285e-02\n",
      " -3.02858595e-02  1.96719705e-03 -6.54954696e-03  7.19525706e-05\n",
      "  4.07770202e-02  6.35513570e-03  2.09778883e-02 -1.80014428e-02\n",
      " -4.28148247e-02  3.75004485e-02 -4.28893119e-02  6.00281358e-03\n",
      " -3.97652648e-02 -4.64697741e-02 -1.27178226e-02 -2.11001337e-02\n",
      "  4.60824892e-02  3.57522070e-02 -1.70442928e-02 -8.01518857e-02\n",
      " -5.05659403e-03 -5.52843176e-02  1.20551502e-02  4.91346838e-03\n",
      " -3.82718421e-03 -5.46370707e-02  5.08492393e-03 -1.02345245e-02\n",
      " -4.26568789e-03  3.83813586e-03  8.85882601e-03 -1.31894648e-02\n",
      " -4.29586172e-02  6.74085394e-02  2.13584006e-02 -5.20860124e-03\n",
      " -1.98980682e-02  2.00310834e-02  2.25171801e-02 -2.60536326e-03\n",
      " -3.85182500e-02 -7.14172097e-03 -6.19293116e-02 -2.29457282e-02\n",
      "  4.87199286e-03 -7.45378137e-02 -8.85656942e-03  1.91385914e-02\n",
      "  3.04827411e-02  5.23110181e-02 -1.02620080e-01 -7.53573105e-02\n",
      "  2.65663862e-02 -1.04825445e-01 -2.89929099e-03  1.68876946e-02\n",
      " -2.18351334e-02 -2.24016905e-02  5.16875461e-02 -6.80863226e-33\n",
      "  5.18745482e-02 -5.75637035e-02  4.19090688e-02  8.37307237e-03\n",
      "  4.18651998e-02  2.14919653e-02 -2.87899617e-02 -1.90830566e-02\n",
      " -1.62059832e-02  1.69678666e-02  8.32210761e-03 -2.10809838e-02\n",
      "  2.60939561e-02 -5.02182916e-02  3.39174941e-02 -1.15264030e-02\n",
      " -2.17170268e-02  1.36721376e-02  2.13254001e-02 -4.67129461e-02\n",
      " -5.63671775e-02  1.92176569e-02 -2.29243888e-03  1.66883729e-02\n",
      "  5.79153523e-02 -3.49393900e-04  1.17923701e-02 -5.84001318e-02\n",
      " -6.06377870e-02 -2.68296786e-02  1.06850090e-02 -3.32936225e-03\n",
      "  4.85855527e-02  4.83674966e-02 -2.51855776e-02 -2.66308226e-02\n",
      "  3.09184808e-02 -8.17612000e-03 -2.77393181e-02 -6.47199601e-02\n",
      " -5.21817394e-02 -2.44164336e-02  2.63256021e-02  3.19269411e-02\n",
      " -2.31747422e-02  4.49921340e-02 -1.82671044e-02 -2.90853400e-02\n",
      "  1.90135036e-02  3.26475576e-02 -2.27008369e-02  1.18862474e-02\n",
      "  6.56675687e-03  1.78174209e-02  1.27114505e-02  1.14019081e-01\n",
      "  7.28865340e-03 -1.02045983e-02  2.74174754e-02  5.75866736e-03\n",
      " -1.86356250e-02  6.05033711e-03  2.57731713e-02  5.15026450e-02\n",
      " -9.67979990e-03 -6.89417347e-02  6.75832946e-03  1.93897486e-02\n",
      " -2.10432652e-02  3.75234382e-03 -2.86727175e-02 -2.70386692e-02\n",
      "  3.41708064e-02  5.84319187e-03 -1.12633789e-02 -9.57165286e-02\n",
      " -2.29099523e-02  5.58980517e-02 -4.16378770e-03 -1.75948292e-02\n",
      " -8.57318789e-02 -3.32208676e-03  3.10157221e-02  1.01991696e-02\n",
      " -3.20784338e-02  2.30804272e-02 -1.16266252e-03  1.91542518e-03\n",
      "  1.88801932e-04 -2.10653152e-02  3.64224962e-03 -1.21830814e-02\n",
      " -5.91113348e-04  3.86593342e-02  5.77500612e-02 -3.97757031e-02\n",
      "  3.00130602e-02 -4.50125709e-02  3.42879482e-02 -3.12360507e-02\n",
      " -7.64033943e-02  2.66280454e-02  3.15062553e-02 -2.07449142e-02\n",
      "  1.59502793e-02 -8.12485740e-02 -2.31788517e-03 -6.38613338e-03\n",
      "  3.89301032e-02 -6.15380891e-02  1.57211572e-02 -2.48287432e-03\n",
      "  4.79475893e-02 -3.62245291e-02 -5.60493506e-02  2.85457056e-02\n",
      "  4.18498889e-02  2.04801597e-02  4.82835770e-02 -3.13245482e-03\n",
      " -1.99085847e-02  1.13255775e-03  2.33066846e-02 -7.09702373e-02\n",
      "  3.75144696e-03 -5.86816706e-02  2.01010592e-02  2.19056867e-02\n",
      " -3.55993584e-02  1.23652886e-03  5.65120243e-02 -1.87019836e-02\n",
      "  3.03676671e-07  2.52633784e-02  5.05705690e-03 -8.72687437e-03\n",
      " -1.22157991e-01 -1.56637467e-02 -3.74373160e-02  4.34251018e-02\n",
      " -7.81771436e-04 -1.78646930e-02 -4.27882187e-03  5.18097803e-02\n",
      " -5.56768365e-02 -3.23175942e-03  8.22806265e-03  1.31896893e-02\n",
      " -2.35082232e-03 -1.26093403e-02 -3.18387188e-02 -3.60387331e-03\n",
      "  9.46238451e-03 -2.37271078e-02  2.89215408e-02 -3.13416906e-02\n",
      " -6.23835102e-02  2.30519800e-03  2.83326972e-02 -2.17426606e-02\n",
      " -5.71741983e-02 -1.43226590e-02 -3.20588909e-02 -6.23973506e-03\n",
      "  1.80297904e-02  4.80441488e-02 -4.87935096e-02 -6.09643087e-02\n",
      " -4.70601022e-02 -1.05718905e-02 -2.60808989e-02 -5.39579168e-02\n",
      "  2.07966045e-02 -4.94423993e-02  3.40963826e-02 -4.12606597e-02\n",
      " -2.46591922e-02 -1.97451469e-02 -2.65906490e-02 -1.76473018e-02\n",
      "  5.78312874e-02  3.47187817e-02 -1.94206089e-02  7.51553616e-03\n",
      "  7.39417411e-03  1.94982942e-02  1.80184282e-02 -2.16851588e-02\n",
      "  1.86842121e-02  4.99627478e-02  3.83541100e-02  4.49745804e-02\n",
      " -6.03930615e-02 -3.52200568e-02  2.30102669e-02 -5.50563782e-02\n",
      " -3.69684882e-02 -2.69742049e-02 -1.21585955e-03  3.38567756e-02\n",
      "  2.66049568e-34 -4.81550358e-02 -1.17253363e-02 -1.34097030e-02\n",
      " -3.95645015e-02  3.51381954e-03  8.25205445e-03 -4.48512807e-02\n",
      "  2.30056942e-02 -1.20406607e-02 -1.02847125e-02  2.27396861e-02]\n"
     ]
    }
   ],
   "source": [
    "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_length]\n",
    "\n",
    "text_chunk_embeddings = embedding_model.encode(\n",
    "    text_chunks,\n",
    "    batch_size=32,\n",
    "    # convert_to_tensor=True\n",
    ")\n",
    "\n",
    "for i, item in tqdm(enumerate(pages_and_chunks_over_min_token_length)):\n",
    "    item[\"embedding\"] = text_chunk_embeddings[i]\n",
    "\n",
    "print(pages_and_chunks_over_min_token_length[1][\"embedding\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save embedding to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_length)\n",
    "# embedding_df_save_path = \"text_chunks_and_embeddings_df.csv\"\n",
    "# text_chunks_and_embeddings_df.to_csv(embedding_df_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_chunks_and_embeddings_df_load = pd.read_csv(embedding_df_save_path)\n",
    "# text_chunks_and_embeddings_df_load.head()\n",
    "# can use an vector database for storage if embedding database is really large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG - Search and Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1680, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df = pd.read_csv(\"text_chunks_and_embeddings_df.csv\")\n",
    "\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[0.0674242601, 0.0902282521, -0.00509550329, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[0.0552156121, 0.059214022, -0.0166167002, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawai‘i at Māno...</td>\n",
       "      <td>766</td>\n",
       "      <td>116</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[0.0279801711, 0.0339814015, -0.020642681, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawai‘i...</td>\n",
       "      <td>941</td>\n",
       "      <td>144</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[0.0682566911, 0.0381274782, -0.00846852828, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai‘...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[0.0330264345, -0.00849768426, 0.00957158767, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawai‘i at Māno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawai‘i...   \n",
       "4          -35  The Cardiovascular System University of Hawai‘...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               116             191.50   \n",
       "3               941               144             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0674242601, 0.0902282521, -0.00509550329, -...  \n",
       "1  [0.0552156121, 0.059214022, -0.0166167002, -0....  \n",
       "2  [0.0279801711, 0.0339814015, -0.020642681, 0.0...  \n",
       "3  [0.0682566911, 0.0381274782, -0.00846852828, -...  \n",
       "4  [0.0330264345, -0.00849768426, 0.00957158767, ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionising the semantic search pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_wrapped(text, width=80):\n",
    "    import textwrap\n",
    "    print(textwrap.fill(text, width=width))\n",
    "    \n",
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                print_time: bool=True):\n",
    "    \n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    start_time = timer() \n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]  # do not need to normalise and use L2 normilisation as the values are already normalised.\n",
    "    end_time = timer()\n",
    "    \n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "    scores, indices = torch.topk(input=dot_scores, \n",
    "                                 k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def retrieve_and_print_top_results(query: str,\n",
    "                                   embeddings: torch.tensor,\n",
    "                                   model: SentenceTransformer = embedding_model,\n",
    "                                   pages_and_chunks: list[dict]= pages_and_chunks,\n",
    "                                   n_resources_to_return: int = 5,\n",
    "                                   print_time: bool = True):\n",
    "    \n",
    "    if model is None:\n",
    "        raise ValueError(\"A SentenceTransformer model must be provided.\")\n",
    "    \n",
    "    if pages_and_chunks is None:\n",
    "        raise ValueError(\"The list 'pages_and_chunks' containing the text chunks must be provided.\")\n",
    "    \n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    \n",
    "    start_time = timer()\n",
    "    dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "    end_time = timer()\n",
    "    \n",
    "    if print_time:\n",
    "        print(f\"[INFO] Time taken to get scores on {len(embeddings)} embeddings: {end_time - start_time:.5f} seconds.\")\n",
    "    \n",
    "    scores, indices = torch.topk(input=dot_scores, k=n_resources_to_return)\n",
    "    \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    \n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\\n\")\n",
    "    \n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Time taken to get scores on 1680 embeddings: 0.05164 seconds.\n",
      "Query: foods high in fibre\n",
      "\n",
      "Results:\n",
      "Score: 0.6866\n",
      "Dietary fiber is categorized as either water-soluble or insoluble. Some examples\n",
      "of soluble fibers are inulin, pectin, and guar gum and they are found in peas,\n",
      "beans, oats, barley, and rye. Cellulose and lignin are insoluble fibers and a\n",
      "few dietary sources of them are whole-grain foods, flax, cauliflower, and\n",
      "avocados. Cellulose is the most abundant fiber in plants, making up the cell\n",
      "walls and providing structure. Soluble fibers are more easily accessible to\n",
      "bacterial enzymes in the large intestine so they can be broken down to a greater\n",
      "extent than insoluble fibers, but even some breakdown of cellulose and other\n",
      "insoluble fibers occurs. The last class of fiber is functional fiber. Functional\n",
      "fibers have been added to foods and have been shown to provide health benefits\n",
      "to humans. Functional fibers may be extracted from plants and purified or\n",
      "synthetically made. An example of a functional fiber is psyllium-seed husk.\n",
      "Scientific studies show that consuming psyllium-seed husk reduces blood-\n",
      "cholesterol levels and this health claim has been approved by the FDA.\n",
      "Page number: 237\n",
      "\n",
      "Score: 0.6661\n",
      "• Change it up a bit and experience the taste and satisfaction of other whole\n",
      "grains such as barley, quinoa, and bulgur. • Eat snacks high in fiber, such as\n",
      "almonds, pistachios, raisins, and air-popped popcorn. Add an artichoke and green\n",
      "peas to your dinner plate more 276 | Carbohydrates and Personal Diet Choices\n",
      "Page number: 276\n",
      "\n",
      "Score: 0.5586\n",
      "Humans and animals store glucose energy from starches in the form of the very\n",
      "large molecule, glycogen. It has many branches that allow it to break down\n",
      "quickly when energy is needed by cells in the body. It is predominantly found in\n",
      "liver and muscle tissue in animals. Dietary Fibers Dietary fibers are\n",
      "polysaccharides that are highly branched and cross-linked. Some dietary fibers\n",
      "are pectin, gums, cellulose, hemicellulose, and lignin. Lignin, however, is not\n",
      "composed of carbohydrate units. Humans do not produce the enzymes that can break\n",
      "down dietary fiber; however, bacteria in the large intestine (colon) do. Dietary\n",
      "fibers are very beneficial to our health. The Dietary Guidelines Advisory\n",
      "Committee states that there is enough scientific evidence to support that diets\n",
      "high in fiber reduce the risk for obesity and diabetes, which are primary risk\n",
      "factors for cardiovascular disease.2 2. US Department of Agriculture.\n",
      "Page number: 236\n",
      "\n",
      "Score: 0.5434\n",
      "potatoes, broccoli, apples, mango, papaya , guavas, blueberries, and\n",
      "strawberries in main and side dishes. Vary your choices to get the benefit of as\n",
      "many different vegetables and fruits as you can. You may choose to drink fruit\n",
      "juice as a replacement for eating fruit. (As long as the juice is 100 percent\n",
      "fruit juice and only half your fruit intake is replaced with juice, this is an\n",
      "acceptable exchange.)For snacks, eat fruits, vegetables, or unsalted nuts. Fill\n",
      "a quarter of your plate with whole grains such as 100 percent whole-grain\n",
      "cereals, breads, crackers, rice, and pasta. Half of your daily grain intake\n",
      "should be whole grains. Read the ingredients list on food labels carefully to\n",
      "determine if a food is comprised of whole grains. Select a variety of protein\n",
      "foods to improve nutrient intake and promote health benefits. Each week, be sure\n",
      "to include a nice array of protein sources in your diet, such as nuts, seeds,\n",
      "beans, legumes, poultry, soy, and seafood.\n",
      "Page number: 747\n",
      "\n",
      "Score: 0.5024\n",
      "Foods GI Value Low GI Foods (< 55) Apple, raw 36 Orange, raw 43 Banana, raw 51\n",
      "Mango, raw 51 Carrots, boiled 39 Taro, boiled 53 Corn tortilla 46 Spaghetti\n",
      "(whole wheat) 37 Baked beans 48 Soy milk 34 Skim milk 37 Whole milk 39 Yogurt,\n",
      "fruit 41 Yogurt, plain 14 Icecream 51 Medium GI Foods (56–69) Pineapple, raw 59\n",
      "Cantaloupe 65 Mashed potatoes 70 Whole-wheat bread 69 Brown rice 55 Cheese pizza\n",
      "60 Sweet potato, boiled 63 Macaroni and cheese 64 Popcorn 65 High GI Foods (70\n",
      "and higher) Banana (over-ripe) 82 Corn chips 72 Digestion and Absorption of\n",
      "Carbohydrates | 249\n",
      "Page number: 249\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.6866, 0.6661, 0.5586, 0.5434, 0.5024], device='mps:0'),\n",
       " tensor([ 360,  418,  358, 1047,  376], device='mps:0'))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"foods high in fibre\"\n",
    "\n",
    "scores, indices = retrieve_and_print_top_results(query=query,\n",
    "                                              embeddings=embeddings)\n",
    "scores, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an LLM locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using model_id: google/gemma-2b-it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berkan_oztas/Bitcoin/env/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01d4e66a33f3478fa792e3dc7c910666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_quantization_config = True  # or False, depending on your use case\n",
    "attn_implementation = \"sdpa\"\n",
    "\n",
    "model_id = \"google/gemma-2b-it\"\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype= torch.float16,\n",
    "                                                 low_cpu_mem_usage = False,\n",
    "                                                 attn_implementation=attn_implementation)\n",
    "\n",
    "if not use_quantization_config:\n",
    "    llm_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2506172416"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5079453696, 'model_mem_mb': 4844.14, 'model_mem_gb': 4.73}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    model_mem_bytes = mem_params + mem_buffers \n",
    "    model_mem_mb = model_mem_bytes / (1024**2) \n",
    "    model_mem_gb = model_mem_bytes / (1024**3) \n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GENERATE TEXT WITH OUR LLM LOCALLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\",\n",
    "    \"How many times a day should a baby be breastfed?\",\n",
    "    \"What are the signs of pellagra?\",\n",
    "    \"What role does saliva play in digestion?\",\n",
    "    \"How much protein should a person consume daily?\",\n",
    "    \"What are examples of water-soluble vitamins?\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str, \n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible for all the options below.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\n",
    "\\nIf you do not know, return general information regarding the query using the context items:\n",
    "{context}\"\"\"\n",
    "\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(query, \n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True, \n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "    \n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "    \n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() \n",
    "        \n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "    \n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cpu\")\n",
    "\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        output_text = output_text.replace(prompt, '').replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "    \n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Describe the process of digestion and absorption of nutrients in the human body.\n",
      "[INFO] Time taken to get scores on 1680 embeddings: 0.00741 seconds.\n",
      "Answer:\n",
      "\n",
      "Sure, here's the answer to your query:\n",
      "\n",
      "The process of digestion and absorption of nutrients in the human body involves several steps that break down food into smaller components that can be absorbed by the body. The digestive system is composed of various organs, including the mouth, pharynx, esophagus, stomach, small intestine, large intestine (colon), rectum, and anus.\n",
      "\n",
      "The process begins with the initial breakdown of food in the mouth, where salivary amylase breaks down starch into smaller molecules. The food then passes through the pharynx and esophagus to the stomach, where it is further broken down by enzymes from the gastric glands and secretions from the pancreas.\n",
      "\n",
      "The digestive system then moves to the small intestine, where the food is further broken down into even smaller molecules. The small intestine is the longest part of the digestive tract, and it is responsible for the majority of nutrient absorption.\n",
      "\n",
      "The digestive system also contains a large number of beneficial bacteria that help to break down food and produce vitamins that the body cannot synthesize on its own. These beneficial bacteria also help to protect the body from harmful pathogens.\n",
      "\n",
      "Overall, the digestive system is a complex and essential process that allows the body to obtain the nutrients it needs to function properly.\n",
      "Page Number: 60\n",
      "Text Chunk: all other organ systems in the human body. We will learn the process of nutrient digestion and absorption, which further reiterates the importance of developing a healthy diet to maintain a healthier you. The evidence abounds that food can indeed be “thy medicine.” Learning Activities Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities. These activities are available in the web-based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or Open Document). Learning activities may be used across various mobile devices, however, for the best user experience it is strongly recommended that users complete these activities using a desktop or laptop computer and in Google Chrome.  An interactive or media element has been excluded from this version of the text. You can view it online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=71 60 | Introduction\n",
      "\n",
      "Page Number: 68\n",
      "Text Chunk: The Digestive System UNIVERSITY OF HAWAI‘I AT MĀNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM The process of digestion begins even before you put food into your mouth. When you feel hungry, your body sends a message to your brain that it is time to eat. Sights and smells influence your body’s preparedness for food. Smelling food sends a message to your brain. Your brain then tells the mouth to get ready, and you start to salivate in preparation for a meal. Once you have eaten, your digestive system (Figure 2.4 “The Human Digestive System”) starts the process that breaks down the components of food into smaller components that can be absorbed and taken into the body. To do this, the digestive system functions on two levels, mechanically to move and mix ingested food and chemically to break down large molecules. The smaller nutrient molecules can then be absorbed and processed by cells throughout the body for energy or used as building blocks for new cells. The digestive system is one of the eleven organ systems of the human body, and it is composed of several hollow tube-shaped organs including the mouth, pharynx, esophagus, stomach, small intestine, large intestine (colon), rectum, and anus. It is lined with mucosal tissue that secretes digestive juices (which aid in the breakdown of food) and mucus (which facilitates the propulsion of food through the tract).\n",
      "\n",
      "Page Number: 252\n",
      "Text Chunk: An interactive or media element has been excluded from this version of the text. You can view it online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=182  An interactive or media element has been excluded from this version of the text. You can view it online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=182 252 | Digestion and Absorption of Carbohydrates\n",
      "\n",
      "Page Number: 80\n",
      "Text Chunk: All eleven organ systems in the human body require nutrient input to perform their specific biological functions. Overall health and the ability to carry out all of life’s basic processes is fueled by energy- supplying nutrients (carbohydrate, fat, and protein). Without them, organ systems would fail, humans would not reproduce, and the race would disappear. In this section, we will discuss some of the critical nutrients that support specific organ system functions. Learning Activities Technology Note: The second edition of the Human Nutrition Open Educational Resource (OER) textbook features interactive learning activities. These activities are available in the web-based textbook and not available in the downloadable versions (EPUB, Digital PDF, Print_PDF, or Open Document). Learning activities may be used across various mobile devices, however, for the best user experience it is strongly recommended that users complete these activities using a desktop or laptop computer and in Google Chrome.  An interactive or media element has been excluded from this version of the text. You can view it online here: 80 | The Digestive System\n",
      "\n",
      "Page Number: 463\n",
      "Text Chunk: Metabolic pathways of a cell down, it is not metabolically efficient for a cell to synthesize fatty acids and break them down at the same time. Catabolism of food molecules begins when food enters the mouth, as the enzyme salivary amylase initiates the breakdown of the starch in foods. The entire process of digestion converts the large polymers in food to monomers that can be absorbed. Starches are broken down to monosaccharides, lipids are broken down to fatty acids, and proteins are broken down to amino acids. These monomers are absorbed into the bloodstream either directly, as is the case with monosaccharides and amino acids, or repackaged in intestinal cells for transport by an indirect route through lymphatic vessels, as is the case with most fatty acids and other fat-soluble molecules. Once absorbed, water-soluble nutrients first travel to the liver which controls their passage into the blood that transports the nutrients to cells throughout the body. The fat-soluble nutrients gradually pass from the lymphatic vessels into blood flowing to body cells. Cells requiring energy or building blocks take up the nutrients from the blood and process them in either catabolic or anabolic pathways. The organ systems of the body require fuel and building blocks to perform the many functions of the body, such as digesting, absorbing, breathing, pumping blood, transporting nutrients in and wastes out, maintaining body temperature, and making new cells. Figure 8.3 Cellular Metabolic Processes The Atom | 463\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "answer, context_items = ask(query=query, \n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print(answer)\n",
    "\n",
    "for item in context_items:\n",
    "    page_number = item['page_number']\n",
    "    sentence_chunk = item['sentence_chunk']\n",
    "    print(f\"Page Number: {page_number}\")\n",
    "    print(f\"Text Chunk: {sentence_chunk}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
